{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 4 Project - Kaggle Competition \"Natural Language Processing with Disaster Tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been accumulated from a number of tweets, some of which are about disasters, some of which are not. By creating a model for Natural Language Processing (NLP), we can predict whether or not a given tweet is about a real disaster or not. This can benefit companies who wish to monitor twitter in the event of an emergency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np \n",
    "import nltk\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd \n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love fruits'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what is NOT a disaster tweet:\n",
    "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what IS a disaster tweet:\n",
    "train_df[train_df[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualizing what proportion of the training data are disaster tweets and non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['target'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = train_df.text\n",
    "y = train_df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text data:\n",
    "need to remove urls, tags (contain @), stopwords, punctuation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5151      this makes sense. paper beats rock paper com...\n",
       "6351    ': the causes of federal failure are deeply st...\n",
       "3443    well as i was chaning an ipad screen it fuckin...\n",
       "7164    the war on drugs has turned the u.s. into a wa...\n",
       "7037    obama declares disaster for typhoondevastated ...\n",
       "5159    according to prophecy and also cnn a mac table...\n",
       "1010    has body bagged ** rt  lac: drake is body bagg...\n",
       "5070     askconnor if you were a natural disaster what...\n",
       "2069     i need you to confirm that ross is dead cause...\n",
       "931      he did get a  'bump' of approval which is pro...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords, punctuation, numbers, and bad characters \n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "no_bad_chars = re.compile('[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n - ]')\n",
    "no_nums = re.compile('[\\d-]')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    #text = no_bad_chars.sub(' ', text) \n",
    "    text = no_nums.sub('', text) \n",
    "    text = re.sub(\"@[A-Za-z0-9]+\",\"\",text) #Remove @ sign\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text) #Remove http links\n",
    "    text = text.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    #text = ' '.join(word for word in text.split() if word not in stopwords_list)\n",
    "    return text\n",
    "    \n",
    "\n",
    "X_train_cleaned = X_train.apply(clean_text)\n",
    "X_test_cleaned = X_test.apply(clean_text)\n",
    "X_train_cleaned.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace smoothing  (attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets = train_df[train_df['target']==1]\n",
    "\n",
    "other_tweets = train_df[train_df['target']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculating the probabilities of disaster and non-disaster tweets in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4296597924602653\n",
      "0.5703402075397347\n"
     ]
    }
   ],
   "source": [
    "P_disasters = len(disaster_tweets) /(len(disaster_tweets)+len(other_tweets))\n",
    "P_non = len(other_tweets) /(len(other_tweets)+len(disaster_tweets))\n",
    "print(P_disasters)\n",
    "print(P_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need set of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = (tuple(nltk.bigrams(X_train,pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr = nltk.bigrams(X_train,pad_left=True, pad_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 5650 conditions>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyDist = nltk.ConditionalFreqDist(bigr)\n",
    "frequencyDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplace smoothing = used to correct probabilities of words so there are no zeroes\n",
    "#categories will be 1 and 0\n",
    "def vocab_maker(category):\n",
    "    vocab_category = set()\n",
    "    \n",
    "    for tweet in category:\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            vocab_category.add(word)\n",
    "    return vocab_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dis = vocab_maker(disaster_tweets['text'])\n",
    "voc_non = vocab_maker(other_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21',\n",
       " 'Ns',\n",
       " 'primarily...',\n",
       " '@traplord_29',\n",
       " 'http://t.co/Ch6E7vTATR',\n",
       " 'http://t.co/ct2JUtvYTg',\n",
       " 'CHONCE',\n",
       " '#AdiosSuperBacterias',\n",
       " 'Force...',\n",
       " 'ET',\n",
       " 'veteran',\n",
       " 'INFO',\n",
       " 'trickshot',\n",
       " 'http://t.co/2o7Eva1cOe',\n",
       " 'blight...',\n",
       " \"Night'\",\n",
       " '#spain',\n",
       " 'Shadowflame',\n",
       " '($)',\n",
       " 'Half',\n",
       " 'Ending',\n",
       " 'proper',\n",
       " 'DROID',\n",
       " 'ETO',\n",
       " '5.139055',\n",
       " 'http://t.co/mFSw0tYstA',\n",
       " 'efforts',\n",
       " 'You)',\n",
       " 'seatbelt!!!...',\n",
       " 'http://t.co/itZzKWfhG5',\n",
       " 'paulista',\n",
       " 'DOWN',\n",
       " 'Wed...',\n",
       " 'http://t.co/R33nCvjovC',\n",
       " '(415)',\n",
       " 'http://t.co/vVE3UsesGf',\n",
       " 'body-bagging',\n",
       " '#US?',\n",
       " 'occupants',\n",
       " '4:30',\n",
       " 'hostage!',\n",
       " '#iJETalerts',\n",
       " 'interactions.',\n",
       " 'scum',\n",
       " 'y',\n",
       " 'harbor',\n",
       " 'roller.',\n",
       " 'burned:',\n",
       " 'RT?',\n",
       " 'standard',\n",
       " 'http://t.co/1RrEO2jG9u',\n",
       " '@accionempresa',\n",
       " 'http://t.co/3bwWNLsxhB',\n",
       " \"bomb'\",\n",
       " '#CNN',\n",
       " '@LindaSOCVAT',\n",
       " 'knew.',\n",
       " 'time)',\n",
       " 'http://t.co/5FcJVMl520',\n",
       " 'SlideShare',\n",
       " '1976.',\n",
       " '2.4RegionåÊåÊNEAR',\n",
       " 'remixes.',\n",
       " 'http://t.co/7IEiZ619h0',\n",
       " 'http://t.co/2kdq56xTWs',\n",
       " '(Black)',\n",
       " 'Nearly',\n",
       " 'Overload',\n",
       " 'rolo',\n",
       " 'explosion.',\n",
       " '@eileenmfl',\n",
       " 'WOULD',\n",
       " \"Pelosi's\",\n",
       " 'disaster?',\n",
       " 'Phase',\n",
       " 'slipping',\n",
       " 'principle',\n",
       " '@fouseyTUBE',\n",
       " 'Prosser',\n",
       " 'http://t.co/7LhKJz0IVO',\n",
       " 'dislocation',\n",
       " 'sinkhole',\n",
       " 'HAIL',\n",
       " 'BBM',\n",
       " 'BOY',\n",
       " 'Finnish',\n",
       " 'Morning',\n",
       " 'Systems',\n",
       " 'rails.',\n",
       " 'http://t.co/wMmkIrJ0Hw',\n",
       " 'Manager',\n",
       " 'meat',\n",
       " 'http://t.co/xe0EE1Fzfh',\n",
       " 'LZKTJNOX',\n",
       " 'http://t.co/LRelVrm06w',\n",
       " './.....hmm',\n",
       " '#NatsNation',\n",
       " 'split',\n",
       " '@AIIAmericanGirI',\n",
       " 'contained',\n",
       " '600!!!',\n",
       " 'http://t.co/GPBXRrDc07',\n",
       " '#CCMusic',\n",
       " '#environment',\n",
       " 'fuck',\n",
       " 'fleek',\n",
       " 'password',\n",
       " 'Frank',\n",
       " 'Maiga',\n",
       " '?????FOLLOWBACK',\n",
       " 'Full',\n",
       " 'lakes',\n",
       " '@Jolly_Jinu',\n",
       " 'Vikings',\n",
       " 'issue?',\n",
       " \"terrorist's\",\n",
       " 'pacific',\n",
       " 'http://t.co/4k8OLZv9bV',\n",
       " 'offensive',\n",
       " 'Stories',\n",
       " \"rts'&amp;'democracy'.\",\n",
       " 'time.',\n",
       " 'Logan',\n",
       " 'Peel',\n",
       " 'reopened.',\n",
       " 'texts',\n",
       " 'ler',\n",
       " 'http://t.co/jCDd6SD6Qn',\n",
       " 'valley',\n",
       " 'tears',\n",
       " 'http://t.co/PyGKSSSCFR',\n",
       " 'Mafia',\n",
       " 'Office',\n",
       " 'lurkin',\n",
       " 'song',\n",
       " 'dominant',\n",
       " 'Humaza.',\n",
       " 'goodlook.Running',\n",
       " 'http://t.co/ZztbVjYPN1',\n",
       " 'Planned',\n",
       " 'Austin.',\n",
       " 'south\\x89Û_',\n",
       " 'solo',\n",
       " '#Indonesia',\n",
       " 'Fuddy',\n",
       " 'count',\n",
       " 'tr...',\n",
       " 'Became',\n",
       " 'http://t.co/gtHddzAvhg',\n",
       " 'http://t.co/TB8gZEMbXU',\n",
       " '#PieceOfMe',\n",
       " 'MUM',\n",
       " 'https://t.co/OAQtjawGxg',\n",
       " '6:08',\n",
       " 'CHPSRE:',\n",
       " 'Locksmithing-art',\n",
       " 'http://t.co/XlFi7ovhFJ',\n",
       " '@jozerphine',\n",
       " 'ultimatum',\n",
       " 'http://t.co/NVfKzv5FEx',\n",
       " '@beelieveDC',\n",
       " 'HALIFAX',\n",
       " 'derailment',\n",
       " 'crash.',\n",
       " 'CONNECTS',\n",
       " 'growing',\n",
       " 'programme',\n",
       " '5:15p',\n",
       " 'http://t.co/ajpbdCalew',\n",
       " 'http://t.co/u4bdy1W7d4',\n",
       " 'Tumblr:',\n",
       " 'nose',\n",
       " 'http://t.co/uCBfgIBFOR',\n",
       " '#sejorg',\n",
       " '#content',\n",
       " 'hair..but',\n",
       " 'Stout)',\n",
       " 'Effects',\n",
       " 'MaFireEMS:',\n",
       " '@PeterDutton_MP',\n",
       " 'upgraded',\n",
       " 'kindermorgan',\n",
       " 'MD',\n",
       " 'bee...',\n",
       " '#INCIDENT',\n",
       " 'https://t.co/EWnUnp8Hdo',\n",
       " 'Ass',\n",
       " '8:00PM',\n",
       " 'http://t.co/9q9Rk3fOf7',\n",
       " 'Greed',\n",
       " 'http://t.co/QVlxpyyyCd',\n",
       " 'KEEP',\n",
       " 'Kisii',\n",
       " 'freaking',\n",
       " 'Hazard',\n",
       " 'Drones',\n",
       " 'mama',\n",
       " 'a\\x89Û_',\n",
       " 'pone',\n",
       " 'v',\n",
       " 'cislady',\n",
       " 'http://t.co/ySpON4d6Qo',\n",
       " 'taint',\n",
       " 'Otherwise',\n",
       " 'artificial',\n",
       " 'dad(who',\n",
       " 'https://t.co/PC3h1NE4G0',\n",
       " 'http://t.co/SDmrzGErYX',\n",
       " 'http://t.co/hHZY3oqeLa',\n",
       " 'http://t.co/NXtWXJCAVh',\n",
       " 'Mesick:',\n",
       " '[In-game]',\n",
       " \"'Islamic\",\n",
       " 'Scourgue',\n",
       " 'returns',\n",
       " '#PPSellsBabyParts',\n",
       " 'ki',\n",
       " 'Freedom:',\n",
       " 'calif',\n",
       " 'Sr.',\n",
       " 'unawares',\n",
       " 'Frozen',\n",
       " 'Transporta...',\n",
       " '(Amsal',\n",
       " 'http://t.co/mkJO8x2dKo',\n",
       " 'TOP',\n",
       " 'http://t.co/S5F9FcOmp8',\n",
       " 'officials',\n",
       " 'impending',\n",
       " 'host:',\n",
       " '#cnbc',\n",
       " 'http://t.co/JlzK2HdeTG',\n",
       " 'Hurricane',\n",
       " 'w/heavenly',\n",
       " 'does',\n",
       " 'powerful!',\n",
       " 'promote',\n",
       " '#artectura',\n",
       " 'http://t.co/UJrX9kgawp',\n",
       " 'liability',\n",
       " 'Flying',\n",
       " 'http://t.co/YS3nMwWyVc',\n",
       " 'Tuffers',\n",
       " '@edsheeran',\n",
       " 'tho...',\n",
       " 'YA',\n",
       " 'Prompting',\n",
       " 'glove.',\n",
       " 'investigation',\n",
       " 'cc:',\n",
       " 'precedent.',\n",
       " 'incompetent.',\n",
       " 'Larger',\n",
       " 'Bank',\n",
       " 'uniform.',\n",
       " 'cousin',\n",
       " 'u',\n",
       " 'Supreme',\n",
       " 'x',\n",
       " 'Lifted',\n",
       " 'Heroine',\n",
       " 'MOVIES',\n",
       " 'Campus',\n",
       " 'psychologist',\n",
       " 'closely',\n",
       " '@CloydRivers',\n",
       " 'Blizzard',\n",
       " 'GTFO',\n",
       " 'flames...',\n",
       " 'courage',\n",
       " 'SNI',\n",
       " 'Dream',\n",
       " '#BB17.',\n",
       " '@Vickie627',\n",
       " 'me...',\n",
       " 'Just',\n",
       " 'Action',\n",
       " 'weights.',\n",
       " 'https://t.co/6Ce1vwOVHs',\n",
       " 'Breeder!',\n",
       " 'http://t.co/Zk69uGXMT8',\n",
       " 'Wheatley',\n",
       " 'seconds.',\n",
       " '@VinusTrip',\n",
       " '@afterShock_DeLo',\n",
       " 'drive-by',\n",
       " 'I-405',\n",
       " 'Boom',\n",
       " 'Career',\n",
       " \"'getting\",\n",
       " '@SoundCloud',\n",
       " 'unarmed',\n",
       " '@igmpj',\n",
       " 'practicing.',\n",
       " 'Sad.',\n",
       " 'Cross-border',\n",
       " '/r/Vaping101',\n",
       " '@AACE_org',\n",
       " 'x1402',\n",
       " 'life??????.',\n",
       " 'TITAN',\n",
       " '@keithboykin',\n",
       " 'Anti-Blight',\n",
       " 'PINER',\n",
       " \"aren't\",\n",
       " 'victim',\n",
       " 'Clearly',\n",
       " 'Two:',\n",
       " 'Crackdown',\n",
       " 'Escape',\n",
       " '24',\n",
       " 'fight?',\n",
       " 'https://t.co/pQHQ4JnZTT',\n",
       " 'cya',\n",
       " 'v...',\n",
       " '10:40',\n",
       " '#newsdict',\n",
       " 'damage.',\n",
       " 'Trends',\n",
       " 'song.',\n",
       " 'http://t.co/fQj0SqU3lG',\n",
       " 'Prone',\n",
       " 'attack..close',\n",
       " 'Newser',\n",
       " '02-06',\n",
       " 'http://t.co/Hz4lKFfC59',\n",
       " 'mueller',\n",
       " 'Sterling',\n",
       " 'wd',\n",
       " 'bot',\n",
       " 'http://t.co/WosYPVQUFI',\n",
       " 'http://t.co/Mphog0QDDN',\n",
       " 'is...',\n",
       " '@optich3cz',\n",
       " \"'twins'\",\n",
       " 'http://t.co/yqpAIjSa5g',\n",
       " 'http://t.co/vxeGCmMVBV',\n",
       " '@Captainn_Morgan',\n",
       " 'VIDEO]',\n",
       " '@gg_keeponrockin',\n",
       " '@DrFriedenCDC',\n",
       " 'UNWARRANTED',\n",
       " '@IndiGo6E',\n",
       " 'Mo\\x89Û_',\n",
       " '#News',\n",
       " 'Justin',\n",
       " 'http://t.co/ZYRZX6dfki',\n",
       " 'cracking',\n",
       " \"Aesthetic'\",\n",
       " 'makes',\n",
       " 'killing',\n",
       " 'http://t.co/ZUqgvJnEQA',\n",
       " 'http://t.co/MZ8VQXbKTs',\n",
       " '@hanna_brooksie',\n",
       " 'ohlordy',\n",
       " \"fine'\",\n",
       " 'spit',\n",
       " 'desolate.',\n",
       " 'http://t.co/l4wJHz4AJ6',\n",
       " '#summer2k15',\n",
       " '#computer',\n",
       " 'https://t.co/OHCx3y8l4s',\n",
       " 'broad',\n",
       " 'Volgagrad',\n",
       " '@ElijahMallari',\n",
       " 'nice.',\n",
       " 'https://t.co/YX1UKbmTqB',\n",
       " 'foreign',\n",
       " 'http://t.co/dDR0zjXVQN',\n",
       " 'ask',\n",
       " 's2g',\n",
       " 'Burns',\n",
       " 'lateral',\n",
       " 'Firefighter',\n",
       " 'http://t.co/l7BJSq0Y2o',\n",
       " 'http://t.co/vn0acCF6D4',\n",
       " 'soz',\n",
       " 'http://t.co/xcGzc45gys',\n",
       " 'G+:',\n",
       " 'Updates:',\n",
       " 'essenceOfMe',\n",
       " '#vaxshill',\n",
       " 'twitch',\n",
       " 'A2&gt;Hanover',\n",
       " 'Lansdowne',\n",
       " 'currently',\n",
       " 'half-railed?',\n",
       " 'Feat.',\n",
       " 'http://t.co/fqcDPhccg7',\n",
       " 'Austin',\n",
       " '@stfxuniversity',\n",
       " 'Yep.',\n",
       " 'grabbed',\n",
       " 'BUILDINGS',\n",
       " 'Korea.',\n",
       " 'http://t.co/dAn0Gkx28l',\n",
       " 'Post',\n",
       " 'heroes...',\n",
       " 'blessed!',\n",
       " 'http://t.co/VMf5LnxVzC',\n",
       " 'phandom',\n",
       " 'recently',\n",
       " 'marinading',\n",
       " 'http://t.co/GeI58Vhbw6',\n",
       " 'joy',\n",
       " 'fence.',\n",
       " 'club',\n",
       " 'http://t.co/HQsU8LWltH',\n",
       " 'http://t.co/RVczMimfVx',\n",
       " 'ruled',\n",
       " 'rude',\n",
       " 'box.',\n",
       " 'http://t.co/RcqacN91bE',\n",
       " 'horror',\n",
       " \"Priority'\",\n",
       " \"'I'm\",\n",
       " 'Spartans',\n",
       " 'http://t.co/d5h4jif1y3',\n",
       " 'Saturday!',\n",
       " 'http://t.co/vVPLFQv58P',\n",
       " 'He',\n",
       " 'lookg',\n",
       " 'since.',\n",
       " 'Forbath',\n",
       " 'Fukushima',\n",
       " 'http://t.co/E3L1JqjH2u',\n",
       " 'http://t.co/7old5MJWph',\n",
       " '@AP',\n",
       " 'http://t.co/RUjV4VPnBV',\n",
       " 'mega',\n",
       " 'ablaze.',\n",
       " 'http://t.co/AQcSUSqbDy',\n",
       " 'Croat',\n",
       " 't/the',\n",
       " 'Volcano',\n",
       " 'StephenSCIFI:',\n",
       " 'Soundtrack',\n",
       " 'mail',\n",
       " 'audiences.',\n",
       " '#Wimbledon',\n",
       " 'Slowly',\n",
       " 'RIGHT',\n",
       " 'gave',\n",
       " 'Freak',\n",
       " '-Lou',\n",
       " 'upcoming',\n",
       " 'Cherry',\n",
       " 'http://t.co/B8iWRdxcm0',\n",
       " 'units',\n",
       " 'fatalities.',\n",
       " 'http://t.co/DXfqOu4kT2',\n",
       " 'mosque',\n",
       " '@mcnabbychic',\n",
       " 'kissing',\n",
       " 'Lit',\n",
       " 'burning:',\n",
       " 'fo...',\n",
       " 'tita-dom:',\n",
       " 'theater:',\n",
       " 'angers',\n",
       " 'Buckley',\n",
       " '@BldrCOSheriff',\n",
       " 'Cabin',\n",
       " 'majority',\n",
       " 'http://t.co/iPHaZV47g7',\n",
       " 'http://t.co/k14q8cHWKp',\n",
       " '@niamhosullivanx',\n",
       " 'Legal',\n",
       " \"'alarmed'\",\n",
       " '@likeavillasboas',\n",
       " 'yes;',\n",
       " 'fold',\n",
       " 'http://t.co/zY3hpdJNwg',\n",
       " 'Finds',\n",
       " 'laughing',\n",
       " 'Briliantly',\n",
       " 'PHONE',\n",
       " 'Editor',\n",
       " 'treasure-house',\n",
       " 'diasporas',\n",
       " 'OUN',\n",
       " '@EyTay',\n",
       " 'nope!!',\n",
       " 'injury.',\n",
       " 'http://t.co/FgDEh56PLO',\n",
       " '#cogXbox',\n",
       " '#Blowltan',\n",
       " 'http://t.co/7nU7pRxeul',\n",
       " '#solicitor',\n",
       " 'quantum',\n",
       " 'heroin',\n",
       " 'incitement',\n",
       " 'http://t.co/nn6Y0fD3l0',\n",
       " 'sucks.',\n",
       " 'Women',\n",
       " 'aboard',\n",
       " '@comcastcares',\n",
       " 'Incase',\n",
       " 'cave',\n",
       " 'Boss',\n",
       " '#diet',\n",
       " 'Nice',\n",
       " '7p.',\n",
       " '#RemyMarcel',\n",
       " '@Hughes1128',\n",
       " 'FWD:',\n",
       " 'power...wow',\n",
       " 'bck',\n",
       " '....bout',\n",
       " 'fav.',\n",
       " 'NEWS',\n",
       " '@stury',\n",
       " 'ducks.',\n",
       " 'recycling',\n",
       " '#destruction',\n",
       " 'w...',\n",
       " '4pm',\n",
       " 'u2',\n",
       " 'Senator',\n",
       " 'Paints',\n",
       " 'stock:',\n",
       " 'Zach',\n",
       " 'heels',\n",
       " 'swear',\n",
       " '4.5%',\n",
       " '@Arovolturi3000',\n",
       " 'U',\n",
       " 'sophistication',\n",
       " 'gmtTy',\n",
       " '#fat',\n",
       " 'ight',\n",
       " '@Jason_Floyd',\n",
       " '@Tim_A_Roberts',\n",
       " 'dougkessler',\n",
       " 'appalling:',\n",
       " 'Daughter',\n",
       " '@Judson1360',\n",
       " 'p.m.',\n",
       " 'http://t.co/X5YEUYLT1X',\n",
       " 'Kurt',\n",
       " 'Hating',\n",
       " 'http://t.co/a3RGQuCUgo',\n",
       " 'Tanzania',\n",
       " '#Clinton',\n",
       " 'kurtkamka:',\n",
       " 'http://t.co/k9FBtcCU58',\n",
       " 'impacting',\n",
       " '@gilmanrocks7',\n",
       " 'that?!',\n",
       " '@SophieWisey',\n",
       " 'hirochii0:',\n",
       " '@attackonstiles',\n",
       " 'Displaced',\n",
       " 'http://t.co/GgnbVZoHWu',\n",
       " '#usa',\n",
       " 'reading;',\n",
       " 'driving.\\x89Û\\x9d',\n",
       " 'http://t.co/zqrcptLrUM',\n",
       " 'http://t.co/ubVEVUuAch',\n",
       " 'intersections',\n",
       " 'action!',\n",
       " '27',\n",
       " 'container',\n",
       " 'LeedStraiF',\n",
       " \"Women's\",\n",
       " 'years*',\n",
       " '@ResignInShame',\n",
       " 'Retail',\n",
       " 'Northland',\n",
       " 'http://t.co/kBe91aRCdw',\n",
       " 'assault',\n",
       " 'http://t.co/ONxhKfHn2a',\n",
       " 'jurors',\n",
       " 'Like!',\n",
       " 'Clico',\n",
       " '\\x89Ûª93',\n",
       " 'tea',\n",
       " '(thus',\n",
       " 'WC',\n",
       " 'Rocket',\n",
       " 'Trains',\n",
       " '#UTFire',\n",
       " 'states',\n",
       " 'bottom',\n",
       " '#1-1ST',\n",
       " 'folks',\n",
       " 'secret',\n",
       " \"memory'\",\n",
       " 'S\\x89ÛªArabia',\n",
       " 'http://t.co/Bgy4i47j70',\n",
       " '#education',\n",
       " 'rifles',\n",
       " 'greinke',\n",
       " 'HamptonRoadsFor.me',\n",
       " 'Media:',\n",
       " 'Helping',\n",
       " 'WEAPON',\n",
       " '@durrellb',\n",
       " 'Then',\n",
       " 'Skylanders',\n",
       " 'Bard',\n",
       " 'recalls',\n",
       " 'been!',\n",
       " 'Differently',\n",
       " 'tonight?!',\n",
       " 'Explode',\n",
       " '@ShekharGupta',\n",
       " 'http://t.co/9Jxb3rx8mF',\n",
       " 'inequality',\n",
       " 'http://t.co/dVONWIv3l1',\n",
       " 'CAREER',\n",
       " 'Christ',\n",
       " 'HAPPINESS',\n",
       " 'Tale',\n",
       " 'Charger',\n",
       " 'buck',\n",
       " 'Ocean',\n",
       " 'plane',\n",
       " 'http://t.co/aPVLH7hj1O',\n",
       " 'https://t.co/WtGGqS5gEh',\n",
       " 'technique.',\n",
       " 'manifestation',\n",
       " '#MyLifeStory',\n",
       " 'Failure',\n",
       " 'guide',\n",
       " 'hype',\n",
       " 'PawSox',\n",
       " '70...&amp;',\n",
       " 'http://t.co/0TSlQjOKvh',\n",
       " 'Tryna',\n",
       " 'Entire',\n",
       " 'AK:',\n",
       " '#blockchain',\n",
       " '@pageparkescorp',\n",
       " 'http://t.co/KcTiGYMahl',\n",
       " 'fifth',\n",
       " 'http://t.co/tCXxHdJAs6',\n",
       " 'line',\n",
       " '#tattoo',\n",
       " 'WN',\n",
       " \"possible.'\",\n",
       " 'Motorcyclist',\n",
       " '&amp;story',\n",
       " '@Rubi_',\n",
       " '23:40:21',\n",
       " 'scam',\n",
       " 'http://t.co/JlWGshYY3N',\n",
       " '#VideoGame',\n",
       " 'beard',\n",
       " 'Tennessee',\n",
       " '#CLIMATE',\n",
       " 'http://t.co/ACZRUOrYtD',\n",
       " '@_chelsdelong12',\n",
       " 'ARA',\n",
       " 'calorie',\n",
       " 'http://t.co/zaRBwep9LD',\n",
       " 'guild',\n",
       " 'WHELEN',\n",
       " 'justified',\n",
       " '#food',\n",
       " 'Errrr',\n",
       " '@bigburgerboi55',\n",
       " 'Lincoln',\n",
       " 'http://t.co/SAkORGdqUL',\n",
       " 'murderer?',\n",
       " 'SIBLING',\n",
       " 'She\\x89Ûªs',\n",
       " 'Wall\\x89Û\\x9d.',\n",
       " '8/10',\n",
       " 'recently.',\n",
       " 'Rosalie',\n",
       " 'http://t.co/TH9YwBbeet',\n",
       " 'want!',\n",
       " 'Josie',\n",
       " 'ma...',\n",
       " 'nine-year-old',\n",
       " 'surge',\n",
       " 'Nueva',\n",
       " 'http://t.co/9asc1hhFNJ',\n",
       " 'https://t.co/zv60cHjclF',\n",
       " 'nudes',\n",
       " 'kou',\n",
       " 'Hobbit:',\n",
       " '[2]',\n",
       " 'lonelyness',\n",
       " 'nostalgia',\n",
       " 'Veld',\n",
       " 'Cash',\n",
       " \"Legionnaires'...\",\n",
       " 'idc',\n",
       " 'Day.',\n",
       " 'multi',\n",
       " 'pleb',\n",
       " 'link:',\n",
       " 'hush',\n",
       " 'http://t.co/bP597YDs2b',\n",
       " 'http://t.co/6mF7eyZOAw',\n",
       " 'Mississippi',\n",
       " 'congratulations',\n",
       " 'vanished',\n",
       " '#man',\n",
       " 'The',\n",
       " 'soon!',\n",
       " 'SkanndTyagi:',\n",
       " '@_AnimalAdvocate',\n",
       " 'purified.',\n",
       " 'remorse',\n",
       " '@InfiniteGrace7',\n",
       " 'sons',\n",
       " 'News@',\n",
       " 'wins??',\n",
       " 'plan.',\n",
       " 'happy.',\n",
       " 'diamond',\n",
       " 'diverts',\n",
       " '#GRupdates',\n",
       " 'Zurich',\n",
       " 'boys',\n",
       " 'Unaware',\n",
       " 'bleeding??',\n",
       " 'Rogue',\n",
       " 'argsuppose',\n",
       " 'http://t.co/m203UL6o7p',\n",
       " 'Kraft',\n",
       " 'tier/',\n",
       " 'infantryman',\n",
       " '#Ices\\x89Û_',\n",
       " 'fighting',\n",
       " '#Helsinki\\x89Û_',\n",
       " 'Flaws',\n",
       " 'BEHIND',\n",
       " 'Hijacker-Turned-SAT-Tutor',\n",
       " 'mask.',\n",
       " 'respects',\n",
       " '#TNN:',\n",
       " 'Cafe.....awful',\n",
       " 'erodes',\n",
       " 'Mizuta...',\n",
       " 'Strikers',\n",
       " \"'Three\",\n",
       " 'Yeahs',\n",
       " 'Manitou,',\n",
       " 'ITUNES',\n",
       " 'http://t.co/Nh5pkFBfqm',\n",
       " 'man.',\n",
       " 'FILM',\n",
       " 'http://t.co/0f8XA4Ih1U',\n",
       " '#TRC',\n",
       " 'Hollywood?',\n",
       " 'Likely',\n",
       " 'http://t.co/DlP8kPkt2k',\n",
       " 'hogging',\n",
       " 'Least',\n",
       " \"WOMEN'S\",\n",
       " \"'historic'\",\n",
       " 'inundated',\n",
       " '24:1',\n",
       " 'http://t.co/CWGCciw3V6',\n",
       " '[TREMOR',\n",
       " 'rubbin',\n",
       " 'journeys',\n",
       " 'http://t.co/DEfJ7XeKgX',\n",
       " 'Chattanooga',\n",
       " 'http://t.co/M9YdA5k6jf',\n",
       " 'Beware',\n",
       " 'Saturn',\n",
       " '@YahooFinance#Hope',\n",
       " 'http://t.co/4ou8s82HxJ',\n",
       " 'Inj',\n",
       " \"they're\",\n",
       " 'YOU',\n",
       " '@alextucker',\n",
       " 'http://t.co/c1nJpLi5oR',\n",
       " \"PRESENT)'\",\n",
       " 'LITERALLY',\n",
       " 'Make',\n",
       " 'days?',\n",
       " '@Pam_Palmater',\n",
       " '10km',\n",
       " 'trick',\n",
       " 'Moments)',\n",
       " 'ouch',\n",
       " 'Control',\n",
       " 'Sparks',\n",
       " '@Annealiz1',\n",
       " '@PhilipDuncan',\n",
       " 'Dozens',\n",
       " 'Harry',\n",
       " '@iamHorsefly',\n",
       " 'Finna',\n",
       " 'OPP.',\n",
       " 'thesensualeye:',\n",
       " \"'Some\",\n",
       " '@FiendNikki',\n",
       " \"'Get\",\n",
       " 'Salvation',\n",
       " '(SJ',\n",
       " 'adventures',\n",
       " 'Hazard/Willian',\n",
       " \"'Crash\",\n",
       " \"'Argentina':\",\n",
       " 'Work',\n",
       " '#prepper',\n",
       " 'soon.',\n",
       " 'EYES',\n",
       " 'chicken',\n",
       " 'died....',\n",
       " 'Kalle',\n",
       " 'up:',\n",
       " \"California's\",\n",
       " 'nuclear',\n",
       " 'Greedy',\n",
       " 'ah',\n",
       " 'tonight\\x89Ûªs',\n",
       " 'on:',\n",
       " 'put',\n",
       " 'happening!',\n",
       " 'West',\n",
       " '\\x89Û÷ALLOOSH',\n",
       " 'http://t.co/nmAUMYdKe1',\n",
       " 'Roger',\n",
       " 'Shed',\n",
       " 'http://t.co/oi6CmAGASi',\n",
       " 'http://t.co/jGdlX4Faw8',\n",
       " \"Iger's\",\n",
       " 'http://t.co/cf9e6TU3g7',\n",
       " 'resort',\n",
       " 'H20',\n",
       " 'inner',\n",
       " 'slow',\n",
       " '90-100.',\n",
       " 'peeps!!',\n",
       " 'dqSVYusY',\n",
       " 'No.2',\n",
       " \"'demolish\",\n",
       " 'healed!!!',\n",
       " 'lightning*',\n",
       " 'days',\n",
       " 'British',\n",
       " 'water??',\n",
       " 'Zergele',\n",
       " 'until?',\n",
       " 'Foothill',\n",
       " 'totaling',\n",
       " 'Tantrums',\n",
       " 'bts',\n",
       " 'against',\n",
       " 'Utd',\n",
       " 'Boise',\n",
       " 'lesson..',\n",
       " 'mockery',\n",
       " 'https://t.co/vwz3vZpmfb',\n",
       " 'fires?',\n",
       " 'http://t.co/dpgdnaoY4p',\n",
       " 'https://t.co/of3td6DGLb',\n",
       " \"Casualty'.\",\n",
       " 'fondness',\n",
       " 'http://t.co/9z9HsmiaVD',\n",
       " 'Imagini',\n",
       " 'http://t.co/UcI8stQUg1',\n",
       " 'SubsD...',\n",
       " '@DannyRaynard',\n",
       " '@kendra_leigh13',\n",
       " 'darkest',\n",
       " 'cold',\n",
       " 'hes',\n",
       " 'https://t.co/ZhJlfLBHZL',\n",
       " 'swallowed.',\n",
       " 'nuff',\n",
       " '#KRO',\n",
       " 'students',\n",
       " 'kids',\n",
       " 'http://t.co/2SZ7oKjRXi',\n",
       " '@adndotcom',\n",
       " 'FANGIRLING',\n",
       " 'lasted',\n",
       " '#FreeALLFour',\n",
       " 'http://t.co/MgR809yc5a',\n",
       " 'boxer',\n",
       " 'http://t.co/nJMiDySXoF',\n",
       " 'Penneys',\n",
       " 'beef',\n",
       " 'Winston',\n",
       " 'oppa',\n",
       " 'http://t.co/QymAlttvZp',\n",
       " 'dept.',\n",
       " 'cartridges...',\n",
       " \"Thursday'\",\n",
       " 'Dajaal?',\n",
       " 'browsing',\n",
       " '@carneross',\n",
       " 'http://t.co/hylMo0WgFI',\n",
       " 'http://t.co/3bRme6Sn4t',\n",
       " 'Hardside',\n",
       " 'call.',\n",
       " 'journalism:',\n",
       " 'OVER!!!',\n",
       " 'Grow',\n",
       " 'NRC_MiddleEast:',\n",
       " 'cultural',\n",
       " 'Original',\n",
       " 'JAPANESE',\n",
       " 'flames!',\n",
       " 'carried',\n",
       " 'SOOOO',\n",
       " 'Parliamentary',\n",
       " 'bruh?',\n",
       " \"'Detonate'\",\n",
       " 'ultimate',\n",
       " 'WORD',\n",
       " 'http://t.co/lY8x7rqbwN',\n",
       " 'LIKE',\n",
       " 'investigation:',\n",
       " 'paeds',\n",
       " \"Higuain's\",\n",
       " 'workplace',\n",
       " 'http://t.co/wHOc7LHb5F',\n",
       " 'court\\x89Ûªs',\n",
       " 'http://t.co/zEVakJaPcz',\n",
       " '-__-',\n",
       " 'Mystery',\n",
       " 'info',\n",
       " 'Signed',\n",
       " 'amazing',\n",
       " 'entrances',\n",
       " 'Sadly',\n",
       " '#mercados',\n",
       " '@_301DC',\n",
       " 'obliterated.',\n",
       " '@kelworldpeace',\n",
       " 'pride',\n",
       " 'Inspiring!',\n",
       " 'xavier',\n",
       " 'enemies!',\n",
       " '#HSE',\n",
       " 'right!',\n",
       " 'modest.',\n",
       " 'southwest',\n",
       " 'cars.',\n",
       " 'skill',\n",
       " '@realdonaldtrump',\n",
       " 'live',\n",
       " 'receiving',\n",
       " 'http://t.co/zM6VcZqvWk',\n",
       " 'http://t.co/6FYnerMUsG',\n",
       " 'subject',\n",
       " '*drools*',\n",
       " 'rooms',\n",
       " 'expander',\n",
       " 'Sharif',\n",
       " 'ferries',\n",
       " 'http://t.co/I9dSPDKrUK',\n",
       " 'rescind',\n",
       " 'Bob',\n",
       " 'Shock',\n",
       " 'landslide',\n",
       " 'firey',\n",
       " 'LINK:',\n",
       " 'Bukidnon:',\n",
       " '@tonycottee1986',\n",
       " 'http://t.co/L3w8miPvnT',\n",
       " 'horses',\n",
       " 'M.O.P.?',\n",
       " 'http://t.co/HaShGQAFic',\n",
       " 'intelligence.',\n",
       " '#golf',\n",
       " 'plz',\n",
       " 'Supply',\n",
       " 'dealbreaker',\n",
       " 'w/soaring',\n",
       " 'O',\n",
       " 'Outbreak?',\n",
       " 'Yosemite',\n",
       " 'M4.',\n",
       " 'day-to-day:',\n",
       " 'evolve.',\n",
       " '##youtube',\n",
       " '513',\n",
       " 'dark.',\n",
       " 'all!!!',\n",
       " 'possible',\n",
       " 'http://t.co/eRJ7YANjXm',\n",
       " 'catastrophic-fallen-angel:',\n",
       " 'DAMAGE;3460',\n",
       " 'Wesley',\n",
       " 'Natalie',\n",
       " 'docked',\n",
       " 'Dress',\n",
       " 'unlocking',\n",
       " 'G20',\n",
       " 'Very',\n",
       " 'idm',\n",
       " 'caution',\n",
       " 'http://t.co/xXOuPfy8nQ',\n",
       " 'soudelor.',\n",
       " 'christie',\n",
       " 'http://t.co/iap4LwvqsW',\n",
       " 'trigger',\n",
       " 'bae',\n",
       " '08/02/15:',\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_all = voc_dis.union(voc_non)\n",
    "voc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-44f8e4ecfb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "\n",
    "train_df['text'].vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_count = len(voc_all)\n",
    "total_dis_count = len(voc_dis)\n",
    "total_non_count = len(voc_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924 16150 20560\n"
     ]
    }
   ],
   "source": [
    "print(total_vocab_count, total_dis_count, total_non_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencyDist = nltk.ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalProbDist with 0 conditions>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilityDist = nltk.ConditionalProbDist(frequencyDist, nltk.LaplaceProbDist, bins=frequencyDist.N())\n",
    "probabilityDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean keywords for better idea of trends\n",
    "def clean_keywords(keyword):\n",
    "    cleaned = re.sub(r'%20', ' ', keyword)\n",
    "    return cleaned\n",
    "def remove_accents(keyword):\n",
    "    cleaned = unidecode.unidecode(keyword)\n",
    "    return cleaned\n",
    "def remove_punctuation(keyword):\n",
    "    cleaned = re.sub(r\"[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n -' ]\",\" \",keyword)\n",
    "    return cleaned\n",
    "#train_df['keyword'].apply(clean_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a48cfa342d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokenize the cleaned tweets data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_sample_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_sample_tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "#tokenize the cleaned tweets data:\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(word)\n",
    "tokenized_sample_tweet = word_tokenize(train_sample)\n",
    "tokenized_sample_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data:\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "regex_tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "X_train_tokenized = X_train_cleaned.copy()\n",
    "X_test_tokenized = X_test_cleaned.copy()\n",
    "X_train_tokenized = X_train_cleaned.apply(tokenizer.tokenize)\n",
    "X_test_tokenized = X_test_cleaned.apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5151    [makes, sense, paper, beats, rock, paper, come...\n",
       "6351    [causes, federal, failure, deeply, structural,...\n",
       "3443    [well, chaning, ipad, screen, fucking, explode...\n",
       "7164                      [war, drugs, turned, war, zone]\n",
       "7037    [obama, declares, disaster, typhoondevastated,...\n",
       "                              ...                        \n",
       "5226    [many, obliteration, servers, always, like, play]\n",
       "5390    [panic, attack, bc, enough, money, drugs, alco...\n",
       "860     [omron, hemc, automatic, blood, pressure, moni...\n",
       "7603    [officials, say, quarantine, place, alabama, h...\n",
       "7270    [moved, england, five, years, ago, today, whir...\n",
       "Name: text, Length: 5709, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Just\n",
       "0        happened\n",
       "0        terrible\n",
       "0             car\n",
       "0           crash\n",
       "          ...    \n",
       "3262          its\n",
       "3262    Municipal\n",
       "3262    Emergency\n",
       "3262         Plan\n",
       "3262     yycstorm\n",
       "Name: text_tokenized, Length: 50940, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep this cell??\n",
    "from nltk import FreqDist\n",
    "\n",
    "train_df[\"text_tokenized\"] = train_df[\"text\"].apply(tokenizer.tokenize)\n",
    "test_df[\"text_tokenized\"] = test_df[\"text\"].apply(tokenizer.tokenize)\n",
    "\n",
    "train_df[\"text_tokenized\"].explode()\n",
    "test_df[\"text_tokenized\"].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "#visualize_top_10(example_freq_dist, \"Top 10 Word Frequency for Example Tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_freq_dist = FreqDist(train_df[\"text_tokenized\"].explode())\n",
    "visualize_top_10(train_freq_dist, \"Top 10 Word Frequency for train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_freq_dist = FreqDist(test_df[\"text_tokenized\"].explode())\n",
    "visualize_top_10(test_freq_dist, \"Top 10 Word Frequency for test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  keyword  location  target  text  text_tokenized\n",
       "0  1.0      0.0       0.0     0.0   0.0             0.0\n",
       "1  0.0      1.0       0.0     0.0   0.0             0.0\n",
       "2  0.0      0.0       1.0     0.0   0.0             0.0\n",
       "3  0.0      0.0       0.0     0.0   1.0             0.0\n",
       "4  0.0      0.0       0.0     1.0   0.0             0.0\n",
       "5  0.0      0.0       0.0     0.0   0.0             1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is this cell necessary?\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "#max_features=None\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "X_test_vectorized = tfidf.transform(X_test)\n",
    "\n",
    "# Which do I use?? The whole train_df or just X_train?\n",
    "\n",
    "train_df_vectorized = tfidf.fit_transform(train_df)\n",
    "test_df_vectorized = tfidf.transform(test_df)\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(train_df_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5151    [dicehateme, PuppyShogun, This, makes, sense, ...\n",
       "6351    [CatoInstitute, The, causes, of, federal, fail...\n",
       "3443    [Well, as, was, chaning, an, iPad, screen, it,...\n",
       "7164    [the, war, on, drugs, has, turned, the, into, ...\n",
       "7037    [Obama, Declares, Disaster, for, Typhoon, Deva...\n",
       "                              ...                        \n",
       "5226    [Eganator2000, There, aren, many, Obliteration...\n",
       "5390    [just, had, panic, attack, bc, don, have, enou...\n",
       "860     [Omron, HEM, 712C, Automatic, Blood, Pressure,...\n",
       "7603    [Officials, say, quarantine, is, in, place, at...\n",
       "7270    [moved, to, England, five, years, ago, today, ...\n",
       "Name: text, Length: 5709, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Madhya',\n",
       " 'Pradesh',\n",
       " 'Train',\n",
       " 'Derailment',\n",
       " 'Village',\n",
       " 'Youth',\n",
       " 'Saved',\n",
       " 'Many',\n",
       " 'Lives']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized.iloc[70][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this section? text is vectorized in pipeline during modeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try count vectorization, then tf-idf ?\n",
    "# Creating a 'bag of words'\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(max_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>co</th>\n",
       "      <th>for</th>\n",
       "      <th>http</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5705</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5709 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      and  co  for  http  in  is  of  the  to  you\n",
       "0       1   0    0     0   0   0   0    0   1    0\n",
       "1       1   1    0     1   0   0   1    1   0    0\n",
       "2       1   0    0     0   0   1   0    1   1    0\n",
       "3       0   0    0     0   0   0   0    2   0    0\n",
       "4       0   0    1     0   0   0   0    0   0    0\n",
       "...   ...  ..  ...   ...  ..  ..  ..  ...  ..  ...\n",
       "5704    0   0    0     0   0   0   0    0   1    0\n",
       "5705    1   0    1     0   0   0   0    1   1    0\n",
       "5706    1   2    0     2   0   0   0    0   0    0\n",
       "5707    0   1    0     1   1   1   0    0   0    0\n",
       "5708    0   1    0     1   0   0   1    0   1    0\n",
       "\n",
       "[5709 rows x 10 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count_vectorized = count_vec.fit_transform(X_train)\n",
    "X_test_count_vectorized = count_vec.transform(X_test)\n",
    "\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_count_vectorized, columns=count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>.</th>\n",
       "      <th>:</th>\n",
       "      <th>?</th>\n",
       "      <th>@</th>\n",
       "      <th>a</th>\n",
       "      <th>http</th>\n",
       "      <th>in</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.792802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362669</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.746706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346645</td>\n",
       "      <td>0.403974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521150</td>\n",
       "      <td>0.607338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.825513</td>\n",
       "      <td>0.291642</td>\n",
       "      <td>0.406374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480049</td>\n",
       "      <td>0.324776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512819</td>\n",
       "      <td>0.362343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5709 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #         .         :    ?         @         a      http        in  \\\n",
       "0     0.0  0.660024  0.000000  0.0  0.660956  0.000000  0.000000  0.000000   \n",
       "1     0.0  0.000000  0.792802  0.0  0.391163  0.000000  0.294835  0.000000   \n",
       "2     0.0  0.746706  0.000000  0.0  0.000000  0.398839  0.000000  0.000000   \n",
       "3     0.0  0.422978  0.000000  0.0  0.000000  0.451852  0.000000  0.000000   \n",
       "4     0.0  0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "...   ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "5704  0.0  0.000000  0.417074  0.0  0.617344  0.000000  0.000000  0.000000   \n",
       "5705  0.0  0.000000  0.000000  0.0  0.000000  0.599619  0.000000  0.000000   \n",
       "5706  0.0  0.000000  0.667451  0.0  0.000000  0.000000  0.744654  0.000000   \n",
       "5707  0.0  0.000000  0.261405  0.0  0.000000  0.825513  0.291642  0.406374   \n",
       "5708  0.0  0.480049  0.324776  0.0  0.000000  0.512819  0.362343  0.000000   \n",
       "\n",
       "           the        to  \n",
       "0     0.000000  0.357079  \n",
       "1     0.362669  0.000000  \n",
       "2     0.346645  0.403974  \n",
       "3     0.785442  0.000000  \n",
       "4     0.000000  0.000000  \n",
       "...        ...       ...  \n",
       "5704  0.000000  0.667035  \n",
       "5705  0.521150  0.607338  \n",
       "5706  0.000000  0.000000  \n",
       "5707  0.000000  0.000000  \n",
       "5708  0.000000  0.519421  \n",
       "\n",
       "[5709 rows x 10 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10, tokenizer=word_tokenize)\n",
    "\n",
    "\n",
    "X_train_vectorized1 = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized1, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolemichaud/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7556707712248865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      1091\n",
      "           1       0.80      0.72      0.76       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.80      0.79      0.79      1904\n",
      "weighted avg       0.80      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "baseline_model = Pipeline([('vect', CountVectorizer(tokenizer=word_tokenize,\n",
    "                                                   stop_words=stopwords_list)),\n",
    "                           ('clf', MultinomialNB())\n",
    "              ])\n",
    "baseline_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = baseline_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolemichaud/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7318087318087317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84      1091\n",
      "           1       0.84      0.65      0.73       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.81      0.78      0.78      1904\n",
      "weighted avg       0.80      0.80      0.79      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model with TF-IDF Vectorizer instead of CountVectorizer, added tfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model2 = Pipeline([('vect', TfidfVectorizer(tokenizer=word_tokenize,\n",
    "                                           stop_words=stopwords_list)),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB()),\n",
    "                  ])\n",
    "model2.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred2 = model2.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred2, y_test))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom tokens\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7500000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      1091\n",
      "           1       0.83      0.68      0.75       813\n",
      "\n",
      "    accuracy                           0.81      1904\n",
      "   macro avg       0.81      0.79      0.80      1904\n",
      "weighted avg       0.81      0.81      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stem_model = Pipeline([('vect', TfidfVectorizer(\n",
    "                         stop_words=stemmed_stopwords,\n",
    "                         tokenizer=stem_and_tokenize)),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "stem_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_stem= stem_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred_stem, y_test))\n",
    "print(classification_report(y_test, y_pred_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What about lemmatization?\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "lemm_stopwords = [lemmatizer.lemmatize(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7415426251691475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84      1091\n",
      "           1       0.82      0.67      0.74       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.81      0.78      0.79      1904\n",
      "weighted avg       0.80      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_model = Pipeline([('vect', TfidfVectorizer(\n",
    "                         stop_words=lemm_stopwords,\n",
    "                         tokenizer=lem_and_tokenize)),\n",
    "                       ('clf', MultinomialNB()),\n",
    "              ])\n",
    "lemm_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_lemm_pred= lemm_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_lemm_pred, y_test))\n",
    "print(classification_report(y_test, y_lemm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_features\": [None ,10, 100, 200],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator TfidfVectorizer(max_features=10,\n                tokenizer=<function word_tokenize at 0x7fe75eab99d0>) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fea795e52f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[0m\u001b[1;32m    655\u001b[0m             self.estimator, scoring=self.scoring)\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    473\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[1;32m    474\u001b[0m                                                           str):\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator TfidfVectorizer(max_features=10,\n                tokenizer=<function word_tokenize at 0x7fe75eab99d0>) does not."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(tfidf , params, cv=3, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train, score='f1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch??\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB() )\n",
    "])\n",
    "\n",
    "# Apparently the only hyperparams for MultinomialNB, but what do they mean?\n",
    "# What about max_features?\n",
    "parameters = {\n",
    "    'alpha': (1, 2, 3),\n",
    "    'class_prior': (),\n",
    "    'fit_prior': ()\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'reg__alpha': (0.00001, 0.000001),\n",
    "    #\"max_features\": [None ,10, 100, 200],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'class_prior', 'fit_prior'])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter max_features for estimator Pipeline(steps=[('vect', TfidfVectorizer()), ('tfidf', TfidfTransformer()),\n                ('clf', MultinomialNB())]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-231b374c92ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[0;34m(self, attr, **params)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# 3. Step parameters and other initialisation arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 raise ValueError('Invalid parameter %s for estimator %s. '\n\u001b[0m\u001b[1;32m    250\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter max_features for estimator Pipeline(steps=[('vect', TfidfVectorizer()), ('tfidf', TfidfTransformer()),\n                ('clf', MultinomialNB())]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords, punctuation, numbers, and bad characters \n",
    "#nltk.download('stopwords', quiet=True)\n",
    "#from nltk.corpus import stopwords\n",
    "#import re\n",
    "\n",
    "#stopwords_list = stopwords.words('english')\n",
    "\n",
    "#no_bad_chars = re.compile('[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n - ]')\n",
    "#no_nums = re.compile('[\\d-]')\n",
    "\n",
    "#def clean_text(text):\n",
    " #   text = text.lower() \n",
    " #   text = no_bad_chars.sub(' ', text) \n",
    " #   text = no_nums.sub('', text) \n",
    " #   \n",
    " #   text = ' '.join(word for word in text.split() if word not in stopwords_list)\n",
    " #   return text\n",
    "    \n",
    "\n",
    "#X_train_cleaned = X_train.apply(clean_text)\n",
    "#X_test_cleaned = X_test.apply(clean_text)\n",
    "#X_train_cleaned.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2fe191bcb84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisasters_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnon_disasters_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "# see vocab for cleaned data\n",
    "train_df_cleaned = train_df['text'].apply(clean_text)\n",
    "\n",
    "disasters_cleaned = train_df_cleaned[train_df_cleaned['target']==1]\n",
    "non_disasters_cleaned = train_df_cleaned[train_df_cleaned['target']==0]\n",
    "\n",
    "voc_dis_cleaned = vocab_maker(disasters_cleaned['text'])\n",
    "voc_non_cleaned = vocab_maker(non_disasters_cleaned['text'])\n",
    "\n",
    "voc_all_cleaned = voc_dis_cleaned.union(voc_non_cleaned)\n",
    "voc_all_cleaned\n",
    "\n",
    "total_vocab_count_cl = len(voc_all_cleaned)\n",
    "total_dis_count_cl = len(voc_dis_cleaned)\n",
    "total_non_count_cl = len(voc_non_cleaned)\n",
    "\n",
    "print(total_vocab_count_cl, total_dis_count_cl, total_non_count_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#model4 = Pipeline([('vect', MultiLabelBinarizer()),\n",
    "               #('clf', MultinomialNB()),\n",
    "              #])\n",
    "#model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#y_pred4 = model4.predict(X_test)\n",
    "\n",
    "#print('accuracy %s' % accuracy_score(y_pred4, y_test))\n",
    "#print(classification_report(y_test, y_pred4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even worse than stemming...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This MultinomialNB estimator requires y to be passed, but the target y is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-38681ec39147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#count_vectorizer = feature_extraction.text.CountVectorizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 return last_step.fit(Xt, y,\n\u001b[0m\u001b[1;32m    379\u001b[0m                                      **fit_params_last_step).transform(Xt)\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'requires_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    417\u001b[0m                     \u001b[0;34mf\"This {self.__class__.__name__} estimator \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This MultinomialNB estimator requires y to be passed, but the target y is None."
     ]
    }
   ],
   "source": [
    "#count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = model2.fit_transform(train_df[\"text\"])\n",
    "test_vectors = model2.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_features_cv = cross_val_score(clf, X_train_vectorized4, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "incr_features_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building baseline model, try things to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-ddaec5e7cabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1879\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "sample_submission[\"target\"] = model2.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_vectorized = tfidf.fit_transform(train_df)\n",
    "test_df_vectorized = tfidf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-83d004593746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               ])\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \"\"\"\n\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#best model:\n",
    "final = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "final.fit(train_df_vectorized, y)\n",
    "\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#y_pred = final.predict(test_df)\n",
    "\n",
    "#print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "#clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(evaluate using f1 metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
