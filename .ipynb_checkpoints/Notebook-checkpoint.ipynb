{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 4 Project - Kaggle Competition \"Natural Language Processing with Disaster Tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been accumulated from a number of tweets, some of which are about disasters, some of which are not. By creating a model for Natural Language Processing (NLP), we can predict whether or not a given tweet is about a real disaster or not. This can benefit companies who wish to monitor twitter in the event of an emergency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "np.random.seed(42)\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split \n",
    "from nltk import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love fruits'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what is NOT a disaster tweet:\n",
    "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what IS a disaster tweet:\n",
    "train_df[train_df[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualizing what proportion of the training data are disaster tweets and non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['target'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text data:\n",
    "need to remove urls, tags (contain @), stopwords, punctuation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         deeds reason earthquake may allah forgive us\n",
       "1                forest fire near la ronge sask canada\n",
       "2    residents asked 'shelter place' notified offic...\n",
       "3    people receive wildfires evacuation orders cal...\n",
       "4    got sent photo ruby alaska smoke wildfires pou...\n",
       "5    rockyfire update california hwy closed directi...\n",
       "6    flood disaster heavy rain causes flash floodin...\n",
       "7                          i'm top hill see fire woods\n",
       "8    there's emergency evacuation happening buildin...\n",
       "9                       i'm afraid tornado coming area\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords, punctuation, numbers, and bad characters \n",
    "# Stopwords removed during pipeline instead ? \n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "no_bad_chars = re.compile('[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n - ]')\n",
    "no_nums = re.compile('[\\d-]')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    text = no_nums.sub('', text) \n",
    "    text = re.sub(\"@[A-Za-z0-9]+\",\"\",text) #Remove @ sign\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text) #Remove http links\n",
    "    text = no_bad_chars.sub(' ', text) \n",
    "    text = text.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords_list)\n",
    "    return text\n",
    "    \n",
    "\n",
    "train_df_cleaned = train_df['text'].apply(clean_text)\n",
    "#train_df_cleaned = pd.DataFrame.sparse.from_spmatrix(train_df_cleaned)\n",
    "test_df_cleaned = test_df['text'].apply(clean_text)\n",
    "#test_df_cleaned = pd.DataFrame.sparse.from_spmatrix(test_df_cleaned)\n",
    "train_df_cleaned.head(10) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace smoothing  (attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first, make bag of words with only top x most common words ?\n",
    "- determine priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculating the probabilities of disaster and non-disaster tweets in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Priors\n",
    "disaster_tweets = train_df[train_df['target']==1]\n",
    "\n",
    "other_tweets = train_df[train_df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4296597924602653\n",
      "0.5703402075397347\n"
     ]
    }
   ],
   "source": [
    "P_disasters = len(disaster_tweets) /(len(disaster_tweets)+len(other_tweets))\n",
    "P_non = len(other_tweets) /(len(other_tweets)+len(disaster_tweets))\n",
    "print(P_disasters)\n",
    "print(P_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that tweets in train_df have a higher probability of not being about a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply Laplace smoothing we add 1 to all word counts:\n",
    "#PLaplace(wi | w(i-1)) = (count(wi w(i-1)) +1 ) / (count(w(i-1)) + V)\n",
    "    #This is for bigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P_disasters_smooth = \n",
    "#P_non_smooth = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothed probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need set of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams = (tuple(nltk.bigrams(X_train,pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigr = nltk.bigrams(X_train,pad_left=True, pad_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencyDist = nltk.ConditionalFreqDist(bigr)\n",
    "#frequencyDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplace smoothing = used to correct probabilities of words so there are no zeroes\n",
    "#categories will be 1 and 0\n",
    "#def vocab_maker(category):\n",
    "#    vocab_category = set()\n",
    "#    \n",
    "#    for tweet in category:\n",
    "#        words = tweet.split()\n",
    "#        for word in words:\n",
    "#            vocab_category.add(word)\n",
    "#    return vocab_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc_dis = vocab_maker(disaster_tweets['text'])\n",
    "#voc_non = vocab_maker(other_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc_all = voc_dis.union(voc_non)\n",
    "#voc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_vocab_count = len(voc_all)\n",
    "#total_dis_count = len(voc_dis)\n",
    "#total_non_count = len(voc_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(total_vocab_count, total_dis_count, total_non_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencyDist = nltk.ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilityDist = nltk.ConditionalProbDist(frequencyDist, nltk.LaplaceProbDist, bins=frequencyDist.N())\n",
    "#probabilityDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean keywords for better idea of trends\n",
    "#def clean_keywords(keyword):\n",
    "#    cleaned = re.sub(r'%20', ' ', keyword)\n",
    "#    return cleaned\n",
    "#def remove_accents(keyword):\n",
    "#    cleaned = unidecode.unidecode(keyword)\n",
    "#    return cleaned\n",
    "#def remove_punctuation(keyword):\n",
    "#    cleaned = re.sub(r\"[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n -' ]\",\" \",keyword)\n",
    "#    return cleaned\n",
    "#train_df['keyword'].apply(clean_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the cleaned tweets data:\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#tokenized_sample_tweet = word_tokenize(train_sample)\n",
    "#tokenized_sample_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data:\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "train_df_tokenized = train_df_cleaned.copy()\n",
    "test_df_tokenized = test_df_cleaned.copy()\n",
    "train_df_tokenized = train_df_cleaned.apply(tokenizer.tokenize)\n",
    "test_df_tokenized = test_df_cleaned.apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deeds, reason, earthquake, may, allah, forgiv...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [residents, asked, shelter, place, notified, o...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [ahrary, control, wild, fires, california, eve...\n",
       "7610                           [utc, km, volcano, hawaii]\n",
       "7611    [police, investigating, ebike, collided, car, ...\n",
       "7612    [latest, homes, razed, northern, california, w...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be performed AFTER Tokenizing data ?\n",
    "\n",
    "X = train_df.text\n",
    "y = train_df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = X_train.apply(clean_text)\n",
    "X_test_cleaned = X_test.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644             new\n",
       "2644          weapon\n",
       "2644           cause\n",
       "2644    unimaginable\n",
       "2644     destruction\n",
       "            ...     \n",
       "1032            blue\n",
       "7195         nuclear\n",
       "7195            bomb\n",
       "7195        terrible\n",
       "7195          weapon\n",
       "Name: text, Length: 16654, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep this cell??\n",
    "\n",
    "X_train[\"text_tokenized\"] = X_train_cleaned.apply(tokenizer.tokenize)\n",
    "X_test[\"text_tokenized\"] = X_test_cleaned.apply(tokenizer.tokenize)\n",
    "\n",
    "X_train[\"text_tokenized\"].explode()\n",
    "X_test[\"text_tokenized\"].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_freq_dist = FreqDist(train_df_tokenized.explode())\n",
    "visualize_top_10(train_freq_dist, \"Top 10 Word Frequency for train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_freq_dist = FreqDist(test_df_tokenized.explode())\n",
    "visualize_top_10(test_freq_dist, \"Top 10 Word Frequency for X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the most common words are the same in both the training df and the test df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to do this visualization for target=1 and target=0 of train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this section? text is vectorized in pipeline during modeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try count vectorization, then tf-idf ?\n",
    "# Creating a 'bag of words'\n",
    "\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vec = CountVectorizer(max_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_count_vectorized = count_vec.fit_transform(X_train)\n",
    "#X_test_count_vectorized = count_vec.transform(X_test)\n",
    "\n",
    "#pd.DataFrame.sparse.from_spmatrix(X_train_count_vectorized, columns=count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = TfidfVectorizer(max_features=10, tokenizer=word_tokenize)\n",
    "\n",
    "\n",
    "#X_train_vectorized1 = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "#pd.DataFrame.sparse.from_spmatrix(X_train_vectorized1, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolemichaud/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7615384615384615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      1091\n",
      "           1       0.80      0.73      0.76       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.80      0.80      0.80      1904\n",
      "weighted avg       0.80      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_model = Pipeline([('vect', CountVectorizer(tokenizer=word_tokenize,\n",
    "                                                   stop_words=stopwords_list)),\n",
    "                           ('clf', MultinomialNB())\n",
    "              ])\n",
    "baseline_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "y_pred = baseline_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolemichaud/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7433264887063654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      1091\n",
      "           1       0.84      0.67      0.74       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.81      0.79      0.79      1904\n",
      "weighted avg       0.81      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model with TF-IDF Vectorizer instead of CountVectorizer\n",
    "\n",
    "model2 = Pipeline([('vect', TfidfVectorizer(tokenizer=word_tokenize,\n",
    "                                           stop_words=stopwords_list)),\n",
    "                   #('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB()),\n",
    "                  ])\n",
    "model2.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred2 = model2.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred2, y_test))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the text to see if it improves our model\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7500000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      1091\n",
      "           1       0.83      0.68      0.75       813\n",
      "\n",
      "    accuracy                           0.81      1904\n",
      "   macro avg       0.81      0.79      0.80      1904\n",
      "weighted avg       0.81      0.81      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Stemmed data model\n",
    "stem_model = Pipeline([('vect', TfidfVectorizer(\n",
    "                         stop_words=stemmed_stopwords,\n",
    "                         tokenizer=stem_and_tokenize)),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "stem_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_stem= stem_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred_stem, y_test))\n",
    "print(classification_report(y_test, y_pred_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What about lemmatization?\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "lemm_stopwords = [lemmatizer.lemmatize(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7415426251691475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84      1091\n",
      "           1       0.82      0.67      0.74       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.81      0.78      0.79      1904\n",
      "weighted avg       0.80      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized data model\n",
    "lemm_model = Pipeline([('vect', TfidfVectorizer(\n",
    "                         stop_words=lemm_stopwords,\n",
    "                         tokenizer=lem_and_tokenize)),\n",
    "                       ('clf', MultinomialNB()),\n",
    "              ])\n",
    "lemm_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_lemm_pred= lemm_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_lemm_pred, y_test))\n",
    "print(classification_report(y_test, y_lemm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these Techniques improved the baseline model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "#X_train_vectorized = vectorizer.fit_transform(train_df[\"text_tokenized\"])\n",
    "#X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Which do I use?? The whole train_df or just X_train?\n",
    "\n",
    "#train_df_vectorized = tfidf.fit_transform(train_df)\n",
    "#test_df_vectorized = tfidf.transform(test_df)\n",
    "\n",
    "#pd.DataFrame.sparse.from_spmatrix(train_df_vectorized, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_vec = cv.fit_transform(X_train_cleaned)\n",
    "X_train_vec  = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(cv.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "\n",
    "\n",
    "X_test_vec = cv.transform(X_test_cleaned)\n",
    "X_test_vec  = pd.DataFrame.sparse.from_spmatrix(X_test_vec)\n",
    "X_test_vec.columns = sorted(cv.vocabulary_)\n",
    "X_test_vec.set_index(y_test.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),\n",
    "#    ('clf', MultinomialNB() )\n",
    "#])\n",
    "\n",
    "# Apparently the only hyperparams for MultinomialNB, but what do they mean?\n",
    "# What about max_features?\n",
    "#parameters = {\n",
    "#    'alpha': [0, 0.1],\n",
    "#    'class_prior': [None, .2, .8],\n",
    "#    'fit_prior': [True, False]\n",
    "#}\n",
    "\n",
    "\n",
    "\n",
    "#grid_search = GridSearchCV(baseline_model, parameters, n_jobs=1, verbose=1, return_train_score=True)\n",
    "#grid_search.fit(X_train_cleaned, y_train, score='f1' )\n",
    "\n",
    "alphas = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "p_grid_NB = {'alpha': alphas, 'fit_prior' : [True, False]}\n",
    "NB_cls= MultinomialNB()\n",
    "\n",
    "grid = GridSearchCV(estimator = NB_cls, param_grid = p_grid_NB, scoring = 'f1', cv = 3)\n",
    "grid.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'fit_prior': True}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolemichaud/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7587499999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      1091\n",
      "           1       0.77      0.75      0.76       813\n",
      "\n",
      "    accuracy                           0.80      1904\n",
      "   macro avg       0.79      0.79      0.79      1904\n",
      "weighted avg       0.80      0.80      0.80      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_model =  Pipeline([('vect', CountVectorizer(tokenizer=word_tokenize,\n",
    "                                                   stop_words=stopwords_list)),\n",
    "                           ('clf', MultinomialNB(alpha= 1.0,fit_prior = True))\n",
    "              ])\n",
    "\n",
    "tuned_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "\n",
    "y_pred_tuned = tuned_model.predict(X_test_cleaned)\n",
    "\n",
    "print('F1 %s' % f1_score(y_pred_tuned, y_test))\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explanation for hyperparams\n",
    "\n",
    "#alpha specifies whether or not to use smoothing/ it is the additive smoothing parameter (0=No smoothing)\n",
    "\n",
    "#class prior is the prior probability of the classes. Default is None.  If specified the priors are not adjusted according to the data.\n",
    "\n",
    "#fit prior specifies whether to learn class prior probabilities or not. If false, a uniform prior will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "model4 = Pipeline([('vect', MultiLabelBinarizer()),\n",
    "               #('clf', MultinomialNB()),\n",
    "              #])\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred4 = model4.predict(X_test)\n",
    "\n",
    "#print('accuracy %s' % accuracy_score(y_pred4, y_test))\n",
    "#print(classification_report(y_test, y_pred4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building baseline model, try things to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-ddaec5e7cabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1879\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "sample_submission[\"target\"] = model2.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-83d004593746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               ])\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \"\"\"\n\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#best model:\n",
    "final = Pipeline([('vect', CountVectorizer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "final.fit(train_df_vectorized, y)\n",
    "\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#y_pred = final.predict(test_df)\n",
    "\n",
    "#print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "#clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(evaluate using f1 metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
